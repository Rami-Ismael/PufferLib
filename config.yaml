### PufferLib demo environments
# Package parameters override defaults.
# Parameters for specific envs override packages
# You cannot specify any deeper than that.
default:
  package: ~
  env_name: ~
  env: {}
  policy: {}
  use_rnn: False
  rnn: {}
  train:
    seed: 1
    torch_deterministic: True
    cpu_offload: False
    device: cuda
    total_timesteps: 10_000_000
    learning_rate: 2.5e-4
    num_steps: 128
    anneal_lr: True
    gamma: 0.99
    gae_lambda: 0.95
    num_minibatches: 4
    update_epochs: 4
    norm_adv: True
    clip_coef: 0.1
    clip_vloss: True
    ent_coef: 0.01
    vf_coef: 0.5
    max_grad_norm: 0.5
    target_kl: ~

    num_envs: 8
    num_workers: 8
    env_batch_size: 8
    zero_copy: True
    verbose: True
    data_dir: experiments
    checkpoint_interval: 200
    batch_size: 1024
    minibatch_size: 512
    bptt_horizon: 16
    vf_clip_coef: 0.1
    compile: False
    compile_mode: reduce-overhead

  sweep:
    method: random
    name: sweep
    metric:
      goal: maximize
      name: environment/episode_return
    # Nested parameters name required by WandB API
    parameters:
      train:
        parameters:
          learning_rate: {
            'distribution': 'log_uniform_values',
            'min': 1e-4,
            'max': 1e-1,
          }
          batch_size: {
            'values': [512, 1024, 2048],
          }
          minibatch_size: {
            'values': [128, 256, 512],
          }
          bptt_horizon: {
            'values': [4, 8, 16],
          }

### Arcade Learning Environment suite
# Convenience wrappers provided for common test environments
atari:
  env_name: BreakoutNoFrameskip-v4
beamrider:
  package: atari
  env_name: BeamRiderNoFrameskip-v4
beam_rider:
  package: atari
  env_name: BeamRiderNoFrameskip-v4
beam-rider:
  package: atari
  env_name: BeamRiderNoFrameskip-v4
breakout:
  package: atari
  env_name: BreakoutNoFrameskip-v4
enduro:
  package: atari
  env_name: EnduroNoFrameskip-v4
pong:
  package: atari
  env_name: PongNoFrameskip-v4
qbert:
  package: atari
  env_name: QbertNoFrameskip-v4
seaquest:
  package: atari
  env_name: SeaquestNoFrameskip-v4
spaceinvaders:
  package: atari
  env_name: SpaceInvadersNoFrameskip-v4
space_invaders:
  package: atari
  env_name: SpaceInvadersNoFrameskip-v4
space-invaders:
  package: atari
  env_name: SpaceInvadersNoFrameskip-v4

breakout-max-sync:
  package: atari
  env_name: BreakoutNoFrameskip-v4
  train:
    num_envs: 48
    num_workers: 24
    env_batch_size: 48
    zero_copy: False
    batch_size: 6144
    minibatch_size: 1536

breakout-max:
  package: atari
  env_name: BreakoutNoFrameskip-v4
  train:
    num_envs: 144 
    num_workers: 24
    env_batch_size: 48
    zero_copy: False
    batch_size: 18432
    minibatch_size: 4608

pong-max:
  package: atari
  env_name: PongNoFrameskip-v4
  train:
    num_envs: 96
    num_workers: 24
    env_batch_size: 48
    zero_copy: False
    batch_size: 65536
    minibatch_size: 2048
 
box2d:
  package: box2d

### Procgen Suite
# Shared hyperparams (best for all envs)
# Per-env hyperparams from CARBS
# Note: These need to be updated for 1.0
# batch sizes likely wrong
procgen:
  env_name: bigfish
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.0005
    num_workers: 24
    num_envs: 144
    env_batch_size: 48
    batch_size: 16384
    minibatch_size: 5321
    gamma: 0.999
    update_epochs: 3
    anneal_lr: False
    clip_coef: 0.2
    vf_clip_coef: 0.2
  policy:
    cnn_width: 16
    mlp_width: 256
bigfish:
  package: procgen
  env_name: bigfish
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.0001901266338648
    gamma: 0.9990684264891424
    ent_coef: 0.0025487710400836
    vf_coef: 1.1732211834792117
    gae_lambda: 0.8620630095238284
    clip_coef: 0.4104603426698214
    batch_size: 53210
    batch_rows: 5321
    bptt_horizon: 1
    update_epochs: 3
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 22
    mlp_width: 327
bossfight:
  package: procgen
  env_name: bossfight
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.0001391202716783
    gamma: 0.9989348776761554
    ent_coef: 0.0141638234842547
    vf_coef: 2.3544979860388664
    gae_lambda: 0.8895733311775463
    clip_coef: 0.5642914060539239
    num_cores: 1
    num_envs: 1
    batch_size: 48520
    minibatch_size: 6065
    update_epochs: 2
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 34
    mlp_width: 83
caveflyer:
  package: procgen
  env_name: caveflyer
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.0003922570060721
    gamma: 0.9974587177630908
    ent_coef: 0.0225727962984408
    vf_coef: 1.6255759569858712
    gae_lambda: 0.9094175213807228
    clip_coef: 0.4508383484491862
    batch_size: 32308
    minibatch_size: 8077
    update_epochs: 2
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 17
    mlp_width: 242
chaser:
  package: procgen
  env_name: chaser
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.0003508035442326
    gamma: 0.9942435848334558
    ent_coef: 0.0071001859366116
    vf_coef: 2.1530812235373684
    gae_lambda: 0.8186838232115529
    clip_coef: 0.0821348744853704
    batch_size: 17456
    minibatch_size: 2182
    update_epochs: 1
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 37
    mlp_width: 198
climber:
  package: procgen
  env_name: climber
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.0001217047694837
    gamma: 0.998084323380632
    ent_coef: 0.0171304566412224
    vf_coef: 0.8123888927054865
    gae_lambda: 0.8758003745828604
    clip_coef: 0.3879433119086241
    batch_size: 113288
    batch_rows: 3332 # ?
    bptt_horizon: 256
    update_epochs: 2
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 29
    mlp_width: 134
coinrun:
  package: procgen
  env_name: coinrun
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.0002171100540455
    gamma: 0.9962953325196714
    ent_coef: 0.0024830293961112
    vf_coef: 0.4045225563446447
    gae_lambda: 0.9708900757395368
    clip_coef: 0.271239381520248
    batch_size: 184170
    minibatch_size: 6139
    update_epochs: 2
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 16
    mlp_width: 384
dodgeball:
  package: procgen
  env_name: dodgeball
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.0002471773711262
    gamma: 0.9892421826991458
    ent_coef: 0.0061212242920176
    vf_coef: 0.905405768115384
    gae_lambda: 0.929215062387182
    clip_coef: 0.1678680070658446
    batch_size: 233026
    minibatch_size: 4958
    update_epochs: 2
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 24
    mlp_width: 538
fruitbot:
  package: procgen
  env_name: fruitbot
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.0005426317191531
    gamma: 0.9988953819963396
    ent_coef: 0.0115430852027873
    vf_coef: 0.5489566038515201
    gae_lambda: 0.7517437269156811
    clip_coef: 0.3909436413913963
    batch_size: 25344
    minibatch_size: 4224
    update_epochs: 1
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 24
    mlp_width: 600
heist:
  package: procgen
  env_name: heist
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.0001460588554421
    gamma: 0.9929899907866796
    ent_coef: 0.0063411167117336
    vf_coef: 1.3750495866441763
    gae_lambda: 0.864713026766495
    clip_coef: 0.0341243664433126
    batch_size: 162233
    minibatch_size: 3061
    update_epochs: 1
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 60
    mlp_width: 154
jumper:
  package: procgen
  env_name: jumper
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.0002667825838749
    gamma: 0.996178793124514
    ent_coef: 0.0035712927399072
    vf_coef: 0.2066134576246479
    gae_lambda: 0.9385007945498072
    clip_coef: 0.0589308261206342
    batch_size: 76925
    minibatch_size: 3077
    update_epochs: 3
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 24
    mlp_width: 190
leaper:
  package: procgen
  env_name: leaper
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.000238551194954
    gamma: 0.9984543257393016
    ent_coef: 0.0264785452036158
    vf_coef: 1.12387183485305
    gae_lambda: 0.8968331903476625
    clip_coef: 0.6941033332120052
    batch_size: 19380
    minibatch_size: 6460
    update_epochs: 2
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 28
    mlp_width: 100
maze:
  package: procgen
  env_name: maze
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.0001711754945436
    gamma: 0.9986484783565428
    ent_coef: 0.0027020733255912
    vf_coef: 0.1236421145384316
    gae_lambda: 0.971943769322524
    clip_coef: 0.2335644352369076
    batch_size: 116008
    minibatch_size: 6834
    update_epochs: 2
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 28
    mlp_width: 526
miner:
  package: procgen
  env_name: miner
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.000328692228852
    gamma: 0.990897931823388
    ent_coef: 0.0045505824544649
    vf_coef: 6.559292234163336
    gae_lambda: 0.6494040942916905
    clip_coef: 0.2293978935956241
    batch_size: 154512
    minibatch_size: 2088
    update_epochs: 3
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 38
    mlp_width: 175
ninja:
  package: procgen
  env_name: ninja
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.0002649776171804
    gamma: 0.998357586821043
    ent_coef: 0.0077158486367147
    vf_coef: 2.171674659769069
    gae_lambda: 0.9664148604540898
    clip_coef: 0.5891635585927152
    batch_size: 45246
    minibatch_size: 7541
    update_epochs: 2
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 25
    mlp_width: 317
plunder:
  package: procgen
  env_name: plunder
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.0002630139944456
    gamma: 0.9981502407071172
    ent_coef: 0.0222691283544936
    vf_coef: 4.316832667738928
    gae_lambda: 0.84500339385464
    clip_coef: 0.0914132500563203
    batch_size: 26304
    minibatch_size: 4384
    update_epochs: 2
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 30
    mlp_width: 288
starpilot:
  package: procgen
  env_name: starpilot
  train:
    total_timesteps: 8_000_000
    learning_rate: 0.0004257280551714
    gamma: 0.9930510505613882
    ent_coef: 0.007836164188961
    vf_coef: 5.482314699746532
    gae_lambda: 0.82792978724664
    clip_coef: 0.2645124138418521
    batch_size: 107440
    minibatch_size: 6715
    update_epochs: 2
    anneal_lr: False
    vf_clip_coef: 0.2
  policy:
    cnn_width: 25
    mlp_width: 144

bsuite:
  package: bsuite
  env_name: bandit/0
  train:
    total_timesteps: 1_000_000
    num_envs: 1

butterfly:
  package: butterfly
  env_name: cooperative_pong_v5

classic_control:
  env_name: cartpole
  train:
    total_timesteps: 500_000
    num_envs: 64
    env_batch_size: 64
classic-control:
  package: classic_control
classiccontrol:
  package: classic_control
cartpole:
  package: classic_control

crafter:
  package: crafter
  env_name: CrafterReward-v1
  train:
    num_envs: 96
    num_workers: 24
    env_batch_size: 48 
    zero_copy: False
    batch_size: 6144
    compile: True

dm_control:
  package: dm_control
dm-control:
  package: dm_control
dmcontrol:
  package: dm_control
dmc:
  package: dm_control

dm_lab:
  package: dm_lab
dm-lab:
  package: dm_lab
dmlab:
  package: dm_lab
dml:
  package: dm_lab

griddly:
  package: griddly
  env_name: GDY-Spiders-v0

magent:
  package: magent
  env_name: battle_v4

microrts:
  env_name: GlobalAgentCombinedRewardEnv

minerl:
  env_name: MineRLNavigateDense-v0

minigrid:
  env_name: MiniGrid-LavaGapS7-v0
  train:
    total_timesteps: 1_000_000
    num_envs: 48
    num_workers: 6
    env_batch_size: 48
    batch_size: 6144
    minibatch_size: 768
    ent_coef: 0.05
    anneal_lr: False

minihack:
  env_name: MiniHack-River-v0
  train:
    num_envs: 48
    num_workers: 24
    env_batch_size: 48
    zero_copy: False
    batch_size: 6144
    minibatch_size: 1536

nethack:
  env_name: NetHackScore-v0
  train:
    num_envs: 72
    num_workers: 24
    env_batch_size: 48
    zero_copy: False
    batch_size: 6144
    update_epochs: 1
    compile: False

nmmo:
  train:
    num_envs: 4
    env_batch_size: 4
    num_workers: 4
    batch_size: 4096
    minibatch_size: 2048

nmmo3:
  use_rnn: True
  train:
    total_timesteps: 10_000_000
    #total_timesteps: 10_000_000_000
    num_envs: 72
    num_workers: 24
    env_batch_size: 24
    update_epochs: 1
    gamma: 0.998
    #batch_size: 32768
    batch_size: 196608
    minibatch_size: 16384
    #compile: True
  env:
    num_envs: 1 # NMMO3 provides its own fast multienv
    #num_envs: 8 # NMMO3 provides its own fast multienv
    #
nmmo3laptop:
  package: nmmo3
  policy:
    inference_batch_size: 4096
    train_batch_size: 16384
  train:
    total_timesteps: 10_000_000
    num_envs: 24
    num_workers: 6
    env_batch_size: 8
    update_epochs: 1
    gamma: 0.998
    batch_size: 32768
    minibatch_size: 16384
    #compile: True
    compile: False

nmmo3debug:
  package: nmmo3
  policy:
    inference_batch_size: 512
    train_batch_size: 8192
  train:
    total_timesteps: 10_000_000
    num_envs: 1
    num_workers: 1
    env_batch_size: 1
    update_epochs: 1
    gamma: 0.998
    #ent_coef: 0.05
    batch_size: 8192
    minibatch_size: 8192
    compile: False
  sweep:
    method: random
    name: sweep
    metric:
      goal: maximize
      name: environment/reward_dist
    # Nested parameters name required by WandB API
    parameters:
      train:
        parameters:
          gamma: {
            'values': [0.95, 0.975, 0.99, 0.995, 0.998],
          }
          learning_rate: {
            'distribution': 'log_uniform_values',
            'min': 1e-4,
            'max': 1e-1,
          }
          batch_size: {
            'values': [8192, 16384, 32768],
          }
          bptt_horizon: {
            'values': [4, 8, 16],
          }



# Ocean: PufferAI's first party environment suite
ocean:
  env_name: squared
  use_rnn: True
  train:
    total_timesteps: 30_000
    learning_rate: 0.017
    num_envs: 8
    num_workers: 2
    env_batch_size: 8
    minibatch_size: 128
    bptt_horizon: 4
    device: cpu
bandit:
  package: ocean
memory:
  package: ocean
multiagent:
  package: ocean
password:
  package: ocean
performance:
  package: ocean
spaces:
  package: ocean
squared:
  package: ocean
stochastic:
  package: ocean

open_spiel:
  env_name: connect_four
  train:
    num_envs: 32
    batch_size: 4096
open-spiel:
  package: open_spiel
openspiel:
  package: open_spiel
connect_four:
  package: open_spiel
  env_name: connect_four
connect-four:
  package: open_spiel
  env_name: connect_four
connectfour:
  package: open_spiel
  env_name: connect_four
connect4:
  package: open_spiel
  env_name: connect_four

pokemon_red:
  use_rnn: True
  train:
    total_timesteps: 1_000_000
    num_envs: 96 
    num_workers: 24
    zero_copy: False
    update_epochs: 3
    gamma: 0.998
    batch_size: 65536
    minibatch_size: 2048
    compile: False 
    learning_rate: 2.0e-4
    anneal_lr: False
    weight_decay: 0.01
pokemon-red:
  package: pokemon_red
pokemonred:
  package: pokemon_red
pokemon:
  package: pokemon_red
pokegym:
  package: pokemon_red
pokedebug:
  package: pokemon_red
  policy:
    mlp_width: 256
    mlp_depth: 1
    add_time: True
  train:
    num_envs: 4
    num_workers: 4
    env_batch_size: 2
    batch_size: 256
    minibatch_size: 256
    compile: False
    optimizer:  Stable_AdamWScheduleFree
    beta1: 0.90
    beta2: 0.995
    continual_learning:
      shrink_and_perturb: False
      shrink : .8
      perturb: .1
    device: cpu
    eps: 1.0e-8
    total_timesteps: 100_000_000
  env:
    random_starter_pokemon: False
    max_episode_steps: 16284  #2048 #16284 #8192 #4096
    display_info_interval_divisor: 256 
    max_reward_clip: 1
    disable_wild_encounters: True
local_cpu_pokemon:
  package: pokemon_red
  use_rnn: True
  env:
    EpisodeStats: False
    max_episode_steps: 256
    display_info_interval_divisor: 128
  train:
    num_envs: 128
    num_workers: 4
    batch_size: 4096
    zero_copy: True
    env_batch_size: 64

    minibatch_size: 64

    clip_vloss: True
    vf_clip_coef: 0.1
    vf_coef: 0.5


    ent_coef: 0.001
    clip_coef: 0.2
    optimizer: Adam




    total_timesteps: 1_000_000_000
    update_epochs: 2
    gamma: 0.998
    anneal_lr: False
    learning_rate: 2.0e-4
    compile: False
    device: cpu
  sweep:
      method: bayes
      name: sweep
      metric:
        goal: maximize
        name: environment/number_of_uniqiue_coordinate_it_explored
      early_terminate: # https://docs.wandb.ai/guides/sweeps/define-sweep-configuration
          type: hyperband
          min_iter: 1
      # Nested parameters name required by WandB API
      parameters:
        train:
          parameters:
            learning_rate: {
              'distribution': 'log_uniform_values',
              'min': 3e-5,
              'max': 2e-4,
            }
puffertank_pokemon_red:
    package: pokemon_red
    use_rnn: True
    policy:
      mlp_width: 1024
      mlp_depth: 3
      embedd_the_x_and_y_coordinate: True
      dense_act_func: "ReLU"
      add_time: True
    env:
      display_info_interval_divisor: 512 
      max_episode_steps: 8192  #2048 #16284 #8192 #4096
      EpisodeStats: False
      max_reward_clip: 1
      reward_for_increase_pokemon_level_coef: .25
      reward_for_explore_unique_coor_coef: .5
      reward_for_increasing_the_highest_pokemon_level_in_the_team_by_battle_coef: 1
      reward_for_entering_a_trainer_battle_coef: 1
      random_starter_pokemon: False
      negative_reward_for_wiping_out_coef: .5
      negative_reward_for_entering_a_trainer_battle_lower_total_pokemon_level_coef: .5
      reward_for_using_bad_moves_coef: 1.0
      disable_wild_encounters: True
    train:
      compile: False
      compile_mode: "reduce-overhead"
      seed: 1
      torch_deterministic: True
      cpu_offload: False
      device: cuda
      total_timesteps: 16_777_216 #268_435_456
      anneal_lr: False 
      gamma: 0.99
      gae_lambda: 0.7
      update_epochs: 16
      norm_adv: True
      clip_coef: 0.2
      ent_coef: 0.1
      clip_vloss: True
      vf_clip_coef: .75
      vf_coef: 0.5
      max_grad_norm: 0.5
      target_kl: .3
      num_envs: 768
      num_workers: 24
      batch_size: 65_536 
      zero_copy: True
      env_batch_size: 256
      env_pool: True

      save_checkpoint: False
      pool_kernel: [0]


      minibatch_size: 16384  # if not set it will be 2048


      learning_rate: 0.0025
      weight_decay: 0.1
      optimizer:  Stable_AdamWScheduleFree
      beta1: 0.9
      beta2: 0.999




      #batch_rows: 512
      bptt_horizon: 16

      continual_learning:
        shrink_and_perturb: False
        shrink : .8
        perturb: .1
    sweep:
        method: bayes
        name: sweep
        metric:
          goal: maximize
          name: environment/number_of_uniqiue_coordinate_it_explored
        early_terminate: # https://docs.wandb.ai/guides/sweeps/define-sweep-configuration
          type: hyperband
          min_iter: 8
          strict: False
          eta: 3
        # Nested parameters name required by WandB API
        parameters:
          train:
            parameters:
              gae_lambda: {
                "values": [.7 , .8 , .9 , .95]
              }
              target_kl: {
                "values": [ .2 , .3  , .4 , .5]
              }
              bptt_horizon: {
                "values": [ 8 , 16 , 32 , 64]
              }
              vf_clip_coef: {
                "values" :[.2 , .3  , .4  , .5 , .6 , .7 , .9 , 1]
              }
              eps: {
                "values": [1.0e-8 , 1.0e-6 , 1.0e-5 , 0.015]
              }
              weight_decay: {
                "values": [0.0 , 0.1 , 0.5 , 0.01 , 0.08]
              }
              beta1: {
                "distribution": "uniform", 
                'min': 0.9 , 
                "max": 0.95
              }
              beta2: { 
                "distribution": "uniform" , 
                "min": 0.75 , 
                "max": 0.99
              }
              learning_rate: {
                "values": [0.1, 0.01, 0.001, 0.0001,  1.0e-05 , 0.0002 , 0.003 , 0.004 , 0.0005 , 0.0025]
              }
              update_epochs: { 
                "values": [ 2 , 4, 8 ]
              }
              clip_coef: {
                "values": [.1 , .2 , .3  , .4]
              }
              ent_coef: {
                "values" : [0 , 0.1 , 0.01]
              }
              batch_size: {
                "values" : [ 16_384, 32768 , 65_536 , 131_072]
              }
          policy:
            parameters:
              mlp_depth: {
               "values": [ 1 , 2 , 3 ]
              }
              mlp_width: {
                "values": [ 256 , 512 , 1024]
              }
          env:
            parameters:
              reward_for_explore_unique_coor_coef: {
                "distribution": "uniform",
                "min":  0.17338864809426202,
                "max": 1.0
              }
              reward_for_entering_a_trainer_battle_coef: {
                "distribution": "uniform" , 
                "min": 0.05,
                "max": 1.0
              }
              negative_reward_for_wiping_out_coef: {
                "distribution": "uniform" ,
                "min": 0.05,
                "max": 1.0
              }
              negative_reward_for_entering_a_trainer_battle_lower_total_pokemon_level_coef: {
                "distribution": "uniform" , 
                "min": 0  , 
                "max": 1.0
              }
              reward_for_using_bad_moves_coef: {
                "distribution": "uniform", 
                "min": 0 , 
                "max": 1.0
              }
              max_episode_steps: {
                "values": [  8192 , 16384 , 32768]
              }
              
links_awaken:
  package: links_awaken
links-awaken:
  package: links_awaken
linksawaken:
  package: links_awaken
zelda:
  package: links_awaken

smac: {}
starcraft:
  package: smac

stable_retro:
  env_name: Airstriker-Genesis
stable-retro:
  package: stable_retro
stableretro:
  package: stable_retro
retro:
  package: stable_retro
