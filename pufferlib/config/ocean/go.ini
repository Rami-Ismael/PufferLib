gbase]
package = ocean
env_name = puffer_go
policy_name = Go
rnn_name = Recurrent

[env]
num_envs = 4096
reward_move_pass = 0
reward_move_valid = 0
reward_move_invalid = -0.3417559868341584
reward_opponent_capture = -0.05
reward_player_capture = 0.05
grid_size = 9
selfplay = 1
komi = 6.5
human_play = 0
[vec]
num_envs = 2
num_workers = 2

[train]
total_timesteps = 10_000_000_000
adam_beta1 = 0.4999999999999999
adam_beta2 = 0.9999842149602696
adam_eps = 9.029178471809546e-13
bptt_horizon = 64
clip_coef = 0.20814799375450627
ent_coef = 0.008468247484984605
gae_lambda = 0.9034361220162442
gamma = 0.9950725864833649
learning_rate = 0.0006375112489476026
min_lr_ratio = 0.060473133238254734
max_grad_norm = 0.9296497485793482
minibatch_size = 16384
prio_alpha = 0.46880396232970556
prio_beta = 0.09999999999999998
vf_clip_coef = 0.1
vf_coef = 1.9597988521012397
vtrace_c_clip = 1.3851087467756749
vtrace_rho_clip = 2.2411082644164178

[sweep.train.total_timesteps]
distribution = log_normal
min = 5e8
max = 3e9
mean = 1e9
scale = 0.25

[sweep.env.reward_move_invalid]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.5
scale = 0.5

[sweep.env.reward_player_capture]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.05
scale = 0.5

[sweep.env.reward_opponent_capture]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.05
scale = 0.5
