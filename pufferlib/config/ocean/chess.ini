[base]
package = ocean
env_name = puffer_chess
policy_name = ChessCNN
rnn_name = Recurrent

[vec]
backend = Multiprocessing
num_envs = 16
num_workers = auto
batch_size = auto

[env]
num_envs = 2048
max_moves = 50000
reward_draw = 0.0
reward_invalid_piece = 0.0
reward_invalid_move = 0.0
reward_valid_piece = 0.0
reward_valid_move = 0.0
reward_material = 0.0
reward_position = 0.0
reward_castling = 0.0
reward_repetition = 0.0
multi_fen = false
enable_50_move_rule = 1
enable_threefold_repetition = 1

[policy]
cnn_channels = 64
hidden_size = 512
use_action_masking = 1

[rnn]
input_size = 512
hidden_size = 512

[train]
seed = 42
torch_deterministic = True
device = cuda
optimizer = muon
anneal_lr = True
min_lr_ratio = 0.1
total_timesteps = 20_000_000_000
learning_rate = 0.003
gamma = 0.99
gae_lambda = 0.95
update_epochs = 1
clip_coef = 0.2
vf_coef = 1.0
vf_clip_coef = 2.0
max_grad_norm = 1.0
ent_coef = 0.05
minibatch_size = 32768
bptt_horizon = 64
adam_beta1 = 0.9
adam_beta2 = 0.999
adam_eps = 1e-8
prio_alpha = 0.8
prio_beta0 = 0.9
vtrace_c_clip = 1.0
vtrace_rho_clip = 1.0

[sweep]
method = Protein
metric = elo
metric_distribution = normal
goal = maximize
downsample = 5
use_gpu = True
prune_pareto = True
early_stop_quantile = 0.3

[sweep.train.total_timesteps]
distribution = log_normal
min = 500e6
max = 2e9
mean = 1e9
scale = time

[sweep.train.learning_rate]
distribution = log_normal
min = 1e-4
max = 1e-2
mean = 3e-3
scale = auto

[sweep.train.min_lr_ratio]
distribution = uniform
min = 0.0
max = 0.2
mean = 0.1
scale = auto

[sweep.train.ent_coef]
distribution = log_normal
min = 0.01
max = 0.15
mean = 0.05
scale = auto

[sweep.train.clip_coef]
distribution = uniform
min = 0.1
max = 0.4
mean = 0.2
scale = auto

[sweep.train.gamma]
distribution = uniform
min = 0.97
max = 0.995
mean = 0.99
scale = auto

[sweep.train.gae_lambda]
distribution = uniform
min = 0.90
max = 0.98
mean = 0.95
scale = auto

[sweep.train.max_grad_norm]
distribution = uniform
min = 0.5
max = 3.0
mean = 1.0
scale = auto

[sweep.train.vf_coef]
distribution = uniform
min = 0.5
max = 4.0
mean = 1.0
scale = auto

[sweep.train.bptt_horizon]
distribution = uniform_pow2
min = 32
max = 128
mean = 64
scale = auto

[sweep.train.minibatch_size]
distribution = uniform_pow2
min = 8192
max = 65536
mean = 32768
scale = auto

[sweep.env.reward_material]
distribution = uniform
min = 0.0
max = 0.005
mean = 0.0
scale = auto

[sweep.env.reward_position]
distribution = uniform
min = 0.0
max = 0.003
mean = 0.0
scale = auto

[sweep.env.reward_castling]
distribution = uniform
min = 0.0
max = 0.005
mean = 0.0
scale = auto

[sweep.env.reward_repetition]
distribution = uniform
min = -0.02
max = 0.0
mean = 0.0
scale = auto
