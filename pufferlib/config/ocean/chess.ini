[base]
package = ocean
env_name = puffer_chess
policy_name = ChessCNN
rnn_name = Recurrent

[vec]
backend = Multiprocessing
num_envs = 8
num_workers = auto
batch_size = auto

[env]
num_envs = 1024
max_moves = 500000
reward_draw = 0.0
reward_invalid_piece = -0.5
reward_invalid_move = -0.5
reward_valid_piece = 0.0
reward_valid_move = 0.0
multi_fen = false
enable_50_move_rule = 1
enable_threefold_repetition = 1

[policy]
cnn_channels = 64
hidden_size = 512

[rnn]
input_size = 512
hidden_size = 512

[train]
seed = 42
torch_deterministic = True
device = cuda
optimizer = muon
anneal_lr = True
total_timesteps = 10_000_000_000
learning_rate = 0.003
gamma = 0.99
gae_lambda = 0.95
update_epochs = 1
clip_coef = 0.2
vf_coef = 2.0
vf_clip_coef = 0.2
max_grad_norm = 1.5
ent_coef = 0.01
minibatch_size = 32768
bptt_horizon = 64

[sweep]
method = Protein
metric = elo
goal = maximize

[sweep.train.learning_rate]
distribution = log_normal
min = 1e-4
max = 1e-2
mean = 3e-3
scale = auto

[sweep.train.ent_coef]
distribution = log_normal
min = 0.001
max = 0.05
mean = 0.01
scale = auto

[sweep.train.clip_coef]
distribution = uniform
min = 0.1
max = 0.3
mean = 0.2
scale = auto

[sweep.train.gamma]
distribution = uniform
min = 0.97
max = 0.995
mean = 0.99
scale = auto

[sweep.train.gae_lambda]
distribution = uniform
min = 0.90
max = 0.98
mean = 0.95
scale = auto

[sweep.train.minibatch_size]
distribution = int_uniform
min = 16384
max = 65536
mean = 32768
scale = auto
