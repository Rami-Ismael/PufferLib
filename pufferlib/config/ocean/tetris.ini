[base]
package = ocean
env_name = puffer_tetris
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 8

[env]
num_envs = 1024
deck_size = 3

[train]
total_timesteps = 1_500_000_000
batch_size = auto
bptt_horizon = 64
minibatch_size = 8192

adam_beta1 = 0.9523796228279156
adam_beta2 = 0.9998539094071076
adam_eps = 0.00000314547896889122
clip_coef = 0.01
ent_coef = 0.0021694177597026067
gae_lambda = 0.6
gamma = 0.9980567485375978
learning_rate = 0.005754066788140346
max_grad_norm = 1.1088295958217178
prio_alpha = 0.9689028428767316
prio_beta0 = 0.7954914636292839
vf_clip_coef = 0.1
vf_coef = 3.2292687796568287
vtrace_c_clip = 2.5378055278019627
vtrace_rho_clip = 1.0246023147360994


[sweep]
metric = score
goal = maximize

[sweep.train.total_timesteps]
distribution = log_normal
min = 30_000_000
max = 3_000_000_000
mean = 200_000_000
scale = auto

[sweep.train.gae_lambda]
distribution = logit_normal
min = 0.01
mean = 0.6
max = 0.995
scale = auto
